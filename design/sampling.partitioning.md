# 2. Sampling
> Step of selecting a small subset of the data to determine how to partition the data on the future step
* identify key valuse that divide the data into approximately equal-sized partitions

## Methods
generated by ChatGPT 4o
1. Systematic Sampling:
    - Selects every nth record from the dataset.
    - Useful when data is evenly distributed and stored in sequential order.
2. Stratified Sampling:
    - Divides the dataset into strata (categories) and then samples within each stratum.
    - Ensures representation of all parts of the dataset, even if the data is skewed.
3. Reservoir Sampling:
    - A specialized technique for sampling when the dataset size is unknown or the data is streamed.
    - Maintains a fixed-size sample while processing the data sequentially.

> Choice: Startifed, Reservoir Sampling, or some other method

## Pseudocode
```scala
// sample structure generated by Copilot

def sample(data: DataFrame, n: Int): DataFrame = {
    // To be implemented
    data.sample(n)
}
```

# 3. Partitioning
> Step of dividing the data into smaller, manageable subsets for processing
* divide the data into partitions based on the key values identified in the sampling step
* determining optimal partition boundaries on Sampling step is crucial 

## Methods
generated by ChatGPT 4o
1. Range-Based Partitioning:
    - Data is divided based on sorted ranges of key values.
    - Suitable for numeric or ordered datasets.
    - Ensures that the final sorted order is easy to merge, as partitions are already sorted by range.
2. Hash-Based Partitioning:
    - A hash function is applied to each key, and the output determines the partition.
    - Useful when the dataset is not inherently ordered (e.g., text data).
    - Does not guarantee globally sorted order unless additional steps are performed.

> Choice: Range-Based Paritioning

## Pseudocode
```scala
// sample structure generated by Copilot

def partition(data: DataFrame, key: String): DataFrame = {
    // To be implemented
    data.partitionBy(key)
}
```